{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NSFB7xCKr4Ll"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KOgainvWthxF"
      },
      "outputs": [],
      "source": [
        "def preprocess():\n",
        "    \"\"\"\n",
        "     Input:\n",
        "     Although this function doesn't have any input, you are required to load\n",
        "     the MNIST data set from file 'mnist_all.mat'.\n",
        "\n",
        "     Output:\n",
        "     train_data: matrix of training set. Each row of train_data contains\n",
        "       feature vector of a image\n",
        "     train_label: vector of label corresponding to each image in the training\n",
        "       set\n",
        "     validation_data: matrix of training set. Each row of validation_data\n",
        "       contains feature vector of a image\n",
        "     validation_label: vector of label corresponding to each image in the\n",
        "       training set\n",
        "     test_data: matrix of training set. Each row of test_data contains\n",
        "       feature vector of a image\n",
        "     test_label: vector of label corresponding to each image in the testing\n",
        "       set\n",
        "    \"\"\"\n",
        "\n",
        "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
        "\n",
        "    n_feature = mat.get(\"train1\").shape[1]\n",
        "    n_sample = 0\n",
        "    for i in range(10):\n",
        "        n_sample = n_sample + mat.get(\"train\" + str(i)).shape[0]\n",
        "    n_validation = 1000\n",
        "    n_train = n_sample - 10 * n_validation\n",
        "\n",
        "    # Construct validation data\n",
        "    validation_data = np.zeros((10 * n_validation, n_feature))\n",
        "    for i in range(10):\n",
        "        validation_data[i * n_validation:(i + 1) * n_validation, :] = mat.get(\"train\" + str(i))[0:n_validation, :]\n",
        "\n",
        "    # Construct validation label\n",
        "    validation_label = np.ones((10 * n_validation, 1))\n",
        "    for i in range(10):\n",
        "        validation_label[i * n_validation:(i + 1) * n_validation, :] = i * np.ones((n_validation, 1))\n",
        "\n",
        "    # Construct training data and label\n",
        "    train_data = np.zeros((n_train, n_feature))\n",
        "    train_label = np.zeros((n_train, 1))\n",
        "    temp = 0\n",
        "    for i in range(10):\n",
        "        size_i = mat.get(\"train\" + str(i)).shape[0]\n",
        "        train_data[temp:temp + size_i - n_validation, :] = mat.get(\"train\" + str(i))[n_validation:size_i, :]\n",
        "        train_label[temp:temp + size_i - n_validation, :] = i * np.ones((size_i - n_validation, 1))\n",
        "        temp = temp + size_i - n_validation\n",
        "\n",
        "    # Construct test data and label\n",
        "    n_test = 0\n",
        "    for i in range(10):\n",
        "        n_test = n_test + mat.get(\"test\" + str(i)).shape[0]\n",
        "    test_data = np.zeros((n_test, n_feature))\n",
        "    test_label = np.zeros((n_test, 1))\n",
        "    temp = 0\n",
        "    for i in range(10):\n",
        "        size_i = mat.get(\"test\" + str(i)).shape[0]\n",
        "        test_data[temp:temp + size_i, :] = mat.get(\"test\" + str(i))\n",
        "        test_label[temp:temp + size_i, :] = i * np.ones((size_i, 1))\n",
        "        temp = temp + size_i\n",
        "\n",
        "    # Delete features which don't provide any useful information for classifiers\n",
        "    sigma = np.std(train_data, axis=0)\n",
        "    index = np.array([])\n",
        "    for i in range(n_feature):\n",
        "        if (sigma[i] > 0.001):\n",
        "            index = np.append(index, [i])\n",
        "    train_data = train_data[:, index.astype(int)]\n",
        "    validation_data = validation_data[:, index.astype(int)]\n",
        "    test_data = test_data[:, index.astype(int)]\n",
        "\n",
        "    # Scale data to 0 and 1\n",
        "    train_data /= 255.0\n",
        "    validation_data /= 255.0\n",
        "    test_data /= 255.0\n",
        "\n",
        "    return train_data, train_label, validation_data, validation_label, test_data, test_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m8av3wZ5tjNN"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pJxkFB7Xtm8n"
      },
      "outputs": [],
      "source": [
        "def blrObjFunction(initialWeights, *args):\n",
        "    \"\"\"\n",
        "    blrObjFunction computes 2-class Logistic Regression error function and\n",
        "    its gradient.\n",
        "\n",
        "    Input:\n",
        "        initialWeights: the weight vector (w_k) of size (D + 1) x 1\n",
        "        train_data: the data matrix of size N x D\n",
        "        labeli: the label vector (y_k) of size N x 1 where each entry can be either 0 or 1 representing the label of corresponding feature vector\n",
        "\n",
        "    Output:\n",
        "        error: the scalar value of error function of 2-class logistic regression\n",
        "        error_grad: the vector of size (D+1) x 1 representing the gradient of\n",
        "                    error function\n",
        "    \"\"\"\n",
        "    train_data, labeli = args\n",
        "\n",
        "    n_data = train_data.shape[0]\n",
        "    n_features = train_data.shape[1]\n",
        "    error = 0\n",
        "    error_grad = np.zeros((n_features + 1, 1))\n",
        "\n",
        "    #add the bias term to your input data\n",
        "\n",
        "    train_data = np.hstack((np.ones((n_data, 1)), train_data))\n",
        "    theta = sigmoid(np.dot(train_data, initialWeights.reshape((n_features + 1, 1))))\n",
        "\n",
        "    error_sum = -np.sum(labeli * np.log(theta) + (1 - labeli) * np.log(1 - theta))\n",
        "    error = error_sum / n_data\n",
        "\n",
        "    error_grad = np.dot(train_data.T, (theta - labeli)) / n_data\n",
        "    error_grad = error_grad.flatten()\n",
        "\n",
        "    return error, error_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0UYn5iSVt3UL"
      },
      "outputs": [],
      "source": [
        "def blrPredict(W, data):\n",
        "    \"\"\"\n",
        "     blrObjFunction predicts the label of data given the data and parameter W\n",
        "     of Logistic Regression\n",
        "\n",
        "     Input:\n",
        "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
        "         vector of a Logistic Regression classifier.\n",
        "         X: the data matrix of size N x D\n",
        "\n",
        "     Output:\n",
        "         label: vector of size N x 1 representing the predicted label of\n",
        "         corresponding feature vector given in data matrix\n",
        "\n",
        "    \"\"\"\n",
        "    label = np.zeros((data.shape[0], 1))\n",
        "\n",
        "    #add the bias term to your input data\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "\n",
        "    data = np.hstack((np.ones((n_data, 1)), data))\n",
        "    label = np.argmax(sigmoid(np.dot(data, W)), axis=1).reshape((n_data, 1))\n",
        "\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7JbU3-ott7D_"
      },
      "outputs": [],
      "source": [
        "def mlrObjFunction(params, *args):\n",
        "    \"\"\"\n",
        "    mlrObjFunction computes multi-class Logistic Regression error function and\n",
        "    its gradient.\n",
        "\n",
        "    Input:\n",
        "        initialWeights_b: the weight vector of size (D + 1) x 10\n",
        "        train_data: the data matrix of size N x D\n",
        "        labeli: the label vector of size N x 1 where each entry can be either 0 or 1\n",
        "                representing the label of corresponding feature vector\n",
        "\n",
        "    Output:\n",
        "        error: the scalar value of error function of multi-class logistic regression\n",
        "        error_grad: the vector of size (D+1) x 10 representing the gradient of\n",
        "                    error function\n",
        "    \"\"\"\n",
        "    train_data, labeli = args\n",
        "    n_data = train_data.shape[0]\n",
        "    n_feature = train_data.shape[1]\n",
        "    error = 0\n",
        "\n",
        "    n_class = labeli.shape[1]\n",
        "\n",
        "    train_data = np.hstack((np.ones((n_data, 1)), train_data))\n",
        "\n",
        "    scores = np.dot(train_data, params.reshape((n_feature + 1, n_class)))\n",
        "    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
        "    probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    error_sum = -np.sum(labeli * np.log(probabilities))\n",
        "    error = error_sum / n_data\n",
        "\n",
        "    error_grad = np.dot(train_data.T, (probabilities - labeli)) / n_data\n",
        "    error_grad = error_grad.flatten()\n",
        "\n",
        "    return error, error_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZB58hK6Zt-QP"
      },
      "outputs": [],
      "source": [
        "def mlrPredict(W, data):\n",
        "    \"\"\"\n",
        "     mlrObjFunction predicts the label of data given the data and parameter W\n",
        "     of Logistic Regression\n",
        "\n",
        "     Input:\n",
        "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
        "         vector of a Logistic Regression classifier.\n",
        "         X: the data matrix of size N x D\n",
        "\n",
        "     Output:\n",
        "         label: vector of size N x 1 representing the predicted label of\n",
        "         corresponding feature vector given in data matrix\n",
        "\n",
        "    \"\"\"\n",
        "    label = np.zeros((data.shape[0], 1))\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data = np.hstack((np.ones((n_data, 1)), data))\n",
        "\n",
        "    scores = np.dot(data, W)\n",
        "    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
        "    probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "    label = np.argmax(probabilities, axis=1).reshape((n_data, 1))\n",
        "\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lduSyW5yr4I3",
        "outputId": "4cb344cb-51b8-4338-c4de-0a6a5db59fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training set Accuracy:92.66799999999999%\n",
            "\n",
            " Validation set Accuracy:91.45%\n",
            "\n",
            " Testing set Accuracy:91.99000000000001%\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Script for Logistic Regression\n",
        "\"\"\"\n",
        "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
        "\n",
        "# number of classes\n",
        "n_class = 10\n",
        "\n",
        "# number of training samples\n",
        "n_train = train_data.shape[0]\n",
        "\n",
        "# number of features\n",
        "n_feature = train_data.shape[1]\n",
        "\n",
        "Y = np.zeros((n_train, n_class))\n",
        "for i in range(n_class):\n",
        "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
        "\n",
        "# Logistic Regression with Gradient Descent\n",
        "W = np.zeros((n_feature + 1, n_class))\n",
        "\n",
        "initialWeights = np.zeros((n_feature + 1,))  # Make it a 1D array\n",
        "\n",
        "opts = {'maxiter': 100}\n",
        "for i in range(n_class):\n",
        "    labeli = Y[:, i].reshape(n_train, 1)\n",
        "    args = (train_data, labeli)\n",
        "    nn_params = minimize(blrObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)\n",
        "    W[:, i] = nn_params.x.reshape((n_feature + 1,))\n",
        "\n",
        "\n",
        "\n",
        "# Find the accuracy on Training Dataset\n",
        "predicted_label = blrPredict(W, train_data)\n",
        "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Validation Dataset\n",
        "predicted_label = blrPredict(W, validation_data)\n",
        "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Testing Dataset\n",
        "predicted_label = blrPredict(W, test_data)\n",
        "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label).astype(float))) + '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGuLEvmvuGL_",
        "outputId": "66c8774a-0bb4-4112-99aa-ca95b812393e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--------------SVM-------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Script for Support Vector Machine\n",
        "\"\"\"\n",
        "\n",
        "print('\\n\\n--------------SVM-------------------\\n\\n')\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "svc_linear = SVC(kernel='linear')\n",
        "svc_linear.fit(train_data, train_label.ravel())\n",
        "\n",
        "svc_rbf_default = SVC(kernel='rbf')\n",
        "svc_rbf_default.fit(train_data, train_label.ravel())\n",
        "\n",
        "svc_rbf_gamma1 = SVC(kernel='rbf', gamma=1)\n",
        "svc_rbf_gamma1.fit(train_data, train_label.ravel())\n",
        "\n",
        "print('\\n Linear Kernel Training Accuracy:', svc_linear.score(train_data, train_label) * 100)\n",
        "print('\\n Linear Kernel Testing Accuracy:', svc_linear.score(test_data, test_label) * 100)\n",
        "print('\\n Linear Kernel Validation Accuracy:', svc_linear.score(validation_data, validation_label) * 100)\n",
        "\n",
        "print('\\n RBF (gamma=1) Training Accuracy:', svc_rbf_gamma1.score(train_data, train_label) * 100)\n",
        "print('\\n RBF (gamma=1) Testing Accuracy:', svc_rbf_gamma1.score(test_data, test_label) * 100)\n",
        "print('\\n RBF (gamma=1) Validation Accuracy:', svc_rbf_gamma1.score(validation_data, validation_label) * 100)\n",
        "\n",
        "print('\\n RBF (default gamma) Training Accuracy:', svc_rbf_default.score(train_data, train_label) * 100)\n",
        "print('\\n RBF (default gamma) Testing Accuracy:', svc_rbf_default.score(test_data, test_label) * 100)\n",
        "print('\\n RBF (default gamma) Validation Accuracy:', svc_rbf_default.score(validation_data, validation_label) * 100)\n",
        "\n",
        "\n",
        "C_values = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "test_accuracy = []\n",
        "\n",
        "for C in C_values:\n",
        "    svc_rbf = SVC(kernel='rbf', C=C)\n",
        "    svc_rbf.fit(train_data, train_label.ravel())\n",
        "    train_accuracy.append(svc_rbf.score(train_data, train_label) * 100)\n",
        "    val_accuracy.append(svc_rbf.score(validation_data, validation_label) * 100)\n",
        "    test_accuracy.append(svc_rbf.score(test_data, test_label) * 100)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(C_values, train_accuracy, label='Training Accuracy', marker='o')\n",
        "plt.plot(C_values, val_accuracy, label='Validation Accuracy', marker='x')\n",
        "plt.plot(C_values, test_accuracy, label='Testing Accuracy', marker='s')\n",
        "plt.xlabel('C value')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Accuracy vs C for RBF Kernel')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m7YNIoksbGY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (.venv)",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}